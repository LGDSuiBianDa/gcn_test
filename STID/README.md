# 背景：
设计一个深度模型，提取MTS场景的长期趋势信息
# 模型架构：
输入嵌入（空间id、时间、事件、历史真值）→ 一维卷积（短期局部特征）→ 时序主干（LSTM+Transformer）→ 多头注意力加权池化 → 流量回归
“以时序为主，空间通过embedding静态注入”，期望从时序主干层学到长期趋势特征**（如周期性、周效应等）**用于后续图神经网络的特征融合。
# 优点:
    时间、空间、事件特征、历史真值充分融入
    层次分明，时序短期（卷积）和长期（RNN+Transformer）都有覆盖
    多头注意力聚合长期依赖，有助于解释和趋势提取
# 不足：
从业务场景来看，“多节点-多序列互动”是必要的：
仅用空间embedding和本身时序能建模“静态空间异质性”，如节点类型特质，但不能捕捉时序上的“跨节点依赖”；
很多真实场景，某节点的补货/发货受“上下游节点/区域中心/邻里节点”的强依赖，需要模型支持时序上多节点的同步或异步特征交互与流动建模。
# 实现“时序上的跨节点依赖”只有图结构吗？
## 1. 图结构是最通用、高效、易解释的手段之一，因为它：
    明确表达了节点间的关系（如邻接、物流路线、上下游流向等）。
    GNN等模型能让每个节点“看到”邻居的特征，做信息传播/依赖提取。
    可以处理异质网络、方向性、多关系等（比如上下游区分边权）。
    支持随时间步/事件变动调整图结构。
## 2. 但并不是“唯一手段”。其他可考虑：
### a. 多维卷积（2D Conv）
    用法：把所有节点的时序拼成[num_warehouses, time_steps]的一个“节点-时间”矩阵（或cube: batch, warehouse, T, F），这样可以像图片一样用卷积核在节点和时间两个维度上滑动、做特征抽取和交互。
    优点：能有效提取临近节点间的时序共性、微局部互动（比如物流带分布式节点），典型如ST-ResNet、交通流量预测等。
    限制：无法灵活建模异构非规则关系（如非欧氏网，又如A连B连Z但不串行），只能捕捉相邻/定距依赖。如果节点在空间上不是"排好队"的，就难以反映真实的上下游关系和流动性。

### b. Attention机制
    可以对所有节点/所有时间步做全局自注意力（multi-head attention），让模型自动学习跨节点、跨时间信息的依赖性，适合大规模分布式系统，参考 T2V（Transformer to Vector）、Informer、SSA（Spatio-Temporal Self-Attention）。
    缺点是算力和内存消耗随节点数和序列长度变大呈平方增长。

# 对于真正“时序上的跨节点依赖”，怎么选方法？
    如果节点之间有明确结构关系（比如物流流、地理图、调货链），强烈建议用图结构GNN类主干，或者注意力+图混合结构。
    如果所有节点如网格均匀排列（比如交通路网、城市区域分布），2D卷积很高效。
    如果所有异质性来源未知或者任意coupling，则全局注意力（Transformer）可用，缺陷就是训大多头时算力压力巨。

# 实例补充

    GNN方案
        每个时间步，对所有节点节点特征同步做图神经网络传播
        每个节点节点在当前/过去邻居的信息都能学到

    2D卷积方案
        node_feats: [B, num_nodes, T, D]
        卷积核如 nn.Conv2d(in_channels=D, out_channels=..., kernel_size=(node_window, time_window))
        只能学到近邻节点的短程依赖，不能区分长链、跳连关系
